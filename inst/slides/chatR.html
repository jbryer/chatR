<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Creating a Custom AI Bot for Your Class</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Bryer, Ph.D." />
    <meta name="date" content="2025-12-05" />
    <script src="libs/header-attrs-2.30/header-attrs.js"></script>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
    <link rel="stylesheet" href="assets/mtheme_max.css" type="text/css" />
    <link rel="stylesheet" href="assets/fonts_mtheme_max.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: center, middle, inverse, title-slide

# Creating a Custom AI Bot for Your Class
## CUNY IT Conference
### Jason Bryer, Ph.D.
### December 5, 2025


---
# Introduction

My motivating question for this project is: **How are students using chatbots?**

To answer this question I created a custom chatbot so...

* Student questions and answers are saved.

* The LLM prioritizes getting from a curated list of resources.

* Data is not sent to any third party (i.e. using local models).

---
# What is AI?

First, AI is a marketing term. I prefer to be more specific regarding what we are doing:

.pull-left[
**Machine Learning (ML)**

This involves training models (e.g. regression, etc.) to predict outcomes. We have been doing this for over a century.
]
.pull-right[
**Large Language Models (LLM)**

This is often what people mean when they say AI. This includes products like ChatGPT, Antrhopic, and Google Gemini. LLMs generate text, images, videos, etc. from a prompt.
]


---
# Outline

1. Document preprocessing - convert our resources (websites, PDFs) to markdown so the LLM can process the text.

2. Text chunking - divide the text into smaller chunks.

3. Embeddings - convert the text into embeddings. These are numeric representations of the text.

4. Storage - Insert the embeddings into a database. We will use [DuckDB](https://duckdb.org) which is a high performance database format designed for data analysis.

5. Connect/Retrieve - connect the knowledge store to our LLM to assist in answer queries.



---
class: font90
# Application Overview

This application is built using [R](https://cran.r-project.org), however minimal knowledge of R is required to setup your application. You can install [R here](https://cran.r-project.org) and [RStudio here](https://posit.co/products/open-source/rstudio/?sid=1). I have more detailed instructions on [my course website](https://fall2025.data606.net/course/software.html)


* [R](https://cran.r-project.org) - Main programming language.

* [Shiny](https://shiny.posit.co) - Framework for creating a web application in R.

* [ellmer](https://ellmer.tidyverse.org) - This package provides functions to interact with many LLM providers, including Ollama.

* [ragnar](https://ragnar.tidyverse.org) - This package assists with the creation of a RAG database.

* [rollama](https://github.com/JBGruber/rollama) - This package is designed to work specifically with Ollama.

* [login](https://github.com/jbryer/login) - R package that adds user authentication to Shiny applications. Recording of a talk I gave on this package at ShinyConf 2024 is available here: https://bryer.org/posts/2024-04-17-ShinyConf2024.html

---
# Local Setup

You first need to install [R](https://cran.r-project.org). I also recommend you install [RStudio](https://posit.co/products/open-source/rstudio/?sid=1). The following command will install the `chatR` package and all dependencies.


``` r
install.packages('pak')
pak::pkg_install('jbryer/chatR')
```

---
# Ollama

.pull-left[
We are going to use Ollama as our LLM engine. We can run this model locally which means student questions are not sent to any thrid party sites.

Download from https://ollama.com

List of available models: https://ollama.com/library

]
.pull-right[
&lt;img src="images/ollama_screenshot.png" width="2432" /&gt;
]


---
# Verifying R can talk to Ollama

First, let's verify that the local Ollama service is running.


``` r
rollama::ping_ollama() # Make sure Ollam is running
```


``` r
model &lt;- 'llama3.1'
```


``` r
rollama::pull_model(model = model)
model_info &lt;- rollama::show_model(model = model)
```

---
# Connecting to the LLM

The `ellmer` package has a number of functions for connecting to *all* the popular LLMs.


``` r
chat &lt;- ellmer::chat_ollama(
	model = model,
	echo = 'all'
)
```

Once we have a `chat` object we can begin sending queries.


``` r
repsonse &lt;- chat$chat('What is the central limit theorem? Keep your answers to less than three sentences.',
					  echo = 'none')
repsonse
```

```
## The Central Limit Theorem (CLT) states that, given certain conditions, the 
## distribution of sample means will approximate a normal distribution even if the
## underlying population distribution is not normal. This allows researchers to 
## use z-scores and standard statistical tools to analyze large datasets that may 
## have come from non-normal populations.
```

---
# Lots of LLMs

This is the current list of available LLMs within the `ellmer` package.


``` r
ls('package:ellmer')[grep('^chat_', ls('package:ellmer'))]
```

```
##  [1] "chat_anthropic"         "chat_aws_bedrock"       "chat_azure_openai"     
##  [4] "chat_claude"            "chat_cloudflare"        "chat_databricks"       
##  [7] "chat_deepseek"          "chat_github"            "chat_google_gemini"    
## [10] "chat_google_vertex"     "chat_groq"              "chat_huggingface"      
## [13] "chat_mistral"           "chat_ollama"            "chat_openai"           
## [16] "chat_openai_compatible" "chat_openrouter"        "chat_perplexity"       
## [19] "chat_portkey"           "chat_snowflake"         "chat_vllm"
```

---
# Retrival Augemented Generated Models

"Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information" ([Wikipedia](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)).

RAG is a way of restricting chatbots to a curated set of resources.

You can optionally allow the chatbot to go beyond the resources if it cannot find an answer in them.

---
# Document preprocessing and chunking

We need to first convert our resources into a format the LLM can understand.


``` r
website_urls &lt;- c('https://r4ds.hadley.nz') # Only using one site for illustrative purposes
pages &lt;- sapply(website_urls, ragnar::ragnar_find_links)
pages &lt;- unlist(pages) |&gt; unname()
```

For all the pages found, convert to markdown and then chunk the text.


``` r
page_chunks &lt;- list()
for(i in 1:length(pages)) {
	page_chunks[[i]] &lt;- pages[i] |&gt; read_as_markdown() |&gt; markdown_chunk()
}
```

---
# Create embeddings and storage

Create the knowledge store (DuckDB database)


``` r
store &lt;- ragnar_store_create(
	'data-raw/ragner_ollama.duckdb,
	embed = \(x) ragnar::embed_ollama(x, model = 'llama3.1')
)
```

Now we can insert our text embeddings into the database.


``` r
for(i in 1:length(page_chunks)) {
	ragnar_store_insert(store, page_chunks[[i]])
}
```

Finnaly, build an index for our knowledge store.


``` r
ragnar::ragnar_store_build_index(store)
```

---
# Connecting to a built knowledge store


``` r
store &lt;- ragnar::ragnar_store_connect(
	'../../data-raw/ragner_ollama.duckdb',
	read_only = FALSE
)
```

---
# Inspecting the knowledge store

**WARNING:** Creating the knowledge store will take a long time. I include 14 sources (3 PDFs, 11 websites) and it took over 12 hours on a M1 MacBook Pro to generate the knowledge store. You can download my knowledge store from the [Github repo](https://github.com/jbryer/chatR/releases).

--

We can retrieve content from the store.


``` r
ragnar::ragnar_retrieve(store, 'bar plots in ggplot2')
```

````
## # A tibble: 5 × 9
##   origin       doc_id chunk_id  start    end cosine_distance bm25  context text 
##   &lt;chr&gt;         &lt;int&gt; &lt;list&gt;    &lt;int&gt;  &lt;int&gt; &lt;list&gt;          &lt;lis&gt; &lt;chr&gt;   &lt;chr&gt;
## 1 https://boo…    335 &lt;int&gt;       567   2614 &lt;dbl [1]&gt;       &lt;dbl&gt; "# [R … "###…
## 2 https://cra…    411 &lt;int&gt;    236774 238580 &lt;dbl [1]&gt;       &lt;dbl&gt; "# An … "`de…
## 3 https://ggp…    134 &lt;int&gt;      4128   6266 &lt;dbl [2]&gt;       &lt;dbl&gt; "# 8  … "```…
## 4 https://ggp…    134 &lt;int&gt;      8780  10506 &lt;dbl [1]&gt;       &lt;dbl&gt; "# 8  … "![]…
## 5 https://mod…    239 &lt;int&gt;     50223  52069 &lt;dbl [1]&gt;       &lt;dbl&gt; "# Cha… "The…
````

The `ragnar` package includes a Shiny application to explore the knowledge store interactively.


``` r
ragnar::ragnar_store_inspect(store)
```



---
# System Prompt

One of the ways we can improve LLMs for our classes is by specifying a system prompt. This is global direction applied to all queries. Here is the system prompt I am currently using for my R and Statistics chatbot:

```
 You are an expert in R and statistics. You are concise.
  Always perform a search of the knowledge store for each user request.
  If the initial search does not return relevant documents, you may perform
  an additional search. Each search will return unique, new excerpts.
  If no relevant results are found, inform the user and do not attempt to answer the question.
  If the user request is ambiguous, perform at least one search first, then ask a clarifying question.
  Every response must cite links to official documentation sources.
```



---
# Connecting the knowledge store to the LLM

Using the `ragnar_register_tool_retrieve` we can connect our knowledge store with any `ellmer` chat object.


``` r
ragnar::ragnar_register_tool_retrieve(chat, store)
```


``` r
chat &lt;- ellmer::chat_ollama(
	model = model, echo = 'all',
	system_prompt = system_prompt)
```



``` r
chat$chat('How do I subset columns from a data frame?')
```

````
## &gt; How do I subset columns from a data frame?
## &lt; You can use the following functions in R to subset columns from a data frame:
## &lt; 
## &lt; 1.  **$**
## &lt; 
## &lt;     *   This operator allows you to select one column by name.
## &lt; 
## &lt;         ```r
## &lt; df$name_col
## &lt; ```
## &lt; 
## &lt;     *   Refrence: [Using `$` and `[[]]` for Accessing Data with 
## &lt; Names](https://stat.ethz.ch/R-manual/R-devel/library/base/html/extract.html)
## &lt; 
## &lt; 2.  **[,]**
## &lt; 
## &lt;     *   This can be used to select columns by position, using a vector of 
## &lt; indices.
## &lt; 
## &lt;         ```r
## &lt; df[, c(1, 3)]
## &lt; ```
## &lt; 
## &lt;     *   Reference: 
## &lt; [Extracting](https://stat.ethz.ch/R-manual/R-devel/library/base/html/extract.html).
## &lt; 
## &lt; Note that it's generally not recommended to simply number the columns where 
## &lt; they are named and may vary.
## &lt; 
## &lt; **Using the dplyr Package**
## &lt; 
## &lt; If you want a more pipeline-like approach, consider using `library(dplyr)` 
## &lt; and selecting with `$` or `[`:
## &lt; 
## &lt; ```r
## &lt; library(dplyr)
## &lt; df %&gt;% select(cols_to_select)
## &lt; ```
## &lt; 
## &lt; *   Reference: [dplyr's select 
## &lt; Function](https://cran.r-project.org/web/packages/dplyr/vignettes/selection.html)
## &lt; 
## &lt; For example:
## &lt; 
## &lt; ```r
## &lt; # A list of columns to select
## &lt; cols_to_select&lt;-c("name", "age","Gender")
## &lt; df %&gt;% dplyr::select( cols_to_select)
## &lt; ```
## &lt; 
## NULL
````

---
class: center, inverse, middle, hide-logo, font140

# DEMO

---
# Demo Locally

You can run the application locally.


``` r
if(!dir.exists('data-raw/')) { dir.create('data-raw/', recursive = TRUE) }
piggyback::pb_download(
	file = "ragner_ollama.duckdb", 
	dest = 'data-raw/',
	repo = "jbryer/chatR",
	tag = "v1.0.0")
chatR::run_chatR(
	store_location = 'data-raw/ragner_ollama.duckdb',
	system_prompt = system_prompt
)
```


---
# Next Steps?

Just finished my first semester using this custom chatbot. Over the break I will begin analyzing in more detail the questions students ask.

I want to also evaluate the accuracy of the answers students received from their questions.

For the Spring semester I intend to solicit more explecit feedback from students on the quality of the chatbot.

I am also interested in how this may related to students responses to one-minute-papers they complete after each class. Students repsond to the following two questions:
1. What was the most important thing you learned during this class?
2. What important question remains unanswered for you?


---
class: center, inverse, middle, hide-logo, font140

# Questions?

---
class: center, inverse, middle, hide-logo, font140

.pull-left[

# Thank You!

.font80[
[&lt;svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M440 6.5L24 246.4c-34.4 19.9-31.1 70.8 5.7 85.9L144 379.6V464c0 46.4 59.2 65.5 86.6 28.6l43.8-59.1 111.9 46.2c5.9 2.4 12.1 3.6 18.3 3.6 8.2 0 16.3-2.1 23.6-6.2 12.8-7.2 21.6-20 23.9-34.5l59.4-387.2c6.1-40.1-36.9-68.8-71.5-48.9zM192 464v-64.6l36.6 15.1L192 464zm212.6-28.7l-153.8-63.5L391 169.5c10.7-15.5-9.5-33.5-23.7-21.2L155.8 332.6 48 288 464 48l-59.4 387.3z"&gt;&lt;/path&gt;&lt;/svg&gt; jason.bryer@cuny.edu](mailto:jason.bryer@cuny.edu)  
[&lt;svg viewBox="0 0 496 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"&gt;&lt;/path&gt;&lt;/svg&gt; @jbryer](https://github.com/jbryer)  
[&lt;svg viewBox="0 0 448 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"&gt;&lt;/path&gt;&lt;/svg&gt; @jbryer@vis.social](https://vis.social/@jbryer)  
[&lt;svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"&gt;&lt;/path&gt;&lt;/svg&gt; www.bryer.org](https://bryer.org)   
]

]
.pull-right[
![](chatR_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "solarized-light",
  "highlightLanguage": "R",
  "highlightLines": true,
  "countIncrementalSlides": false,
  "ratio": "16:9",
  "navigation": {
    "scroll": false
  }
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
